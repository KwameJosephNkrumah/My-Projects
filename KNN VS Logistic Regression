---
title: "KNN vs LOGISTIC REGRESSION"
author: "KWAME NKRUMAH"

output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ANNOTATED CODES

## Section 1: Introduction and Data

 
```{r ,warning=FALSE}

#Importing Data

suppressMessages(library(readr))
dengueData <- read_csv("dengueData.csv",show_col_types = FALSE)

#Cleaning dengueData
dengueDataClean <- na.omit(dengueData)

```

## Section 2: KNN


```{r ,fig.asp = .7}
## Create a scatter plot using age and height only from the training data and choosing k=3 to show how to predict for a child who is age 8 and is 100 cm tall. 
library(ggplot2)
ggplot(dengueDataClean, aes(x=Age, y = Height, color = Y)) + geom_point()+geom_point(aes(x=8,y=100),colour="red") +labs(title="Figure 2.1:", x="Age (in years)", y="Height(in cm)")




```



```{r}
## Creating a 20% test, 80% training split of the dengueDataClean

# Count how may  observations we have
n <- nrow(dengueDataClean)
# Set a random seed
set.seed(123)
# Sample the rows
rowsdengueDataClean <- sample(1:n, n*.8, replace = FALSE)
# Create the  training data
TrainingData <- dengueDataClean[rowsdengueDataClean, ]

# Create the  test data
TestData <- dengueDataClean[-rowsdengueDataClean, ]


```



```{r, fig.asp = .4}
#Creating a 20% test, 80% training split of the data
##Determine how many choices of k
par(mfrow = c(2, 1))
nK <- 50
set.seed(123)

##Create any storage spaces
storagespace <- data.frame("K"= rep(NA,nK),"GMean"= rep(NA,nK)) 
##Determine the true positives and true negatives
truePositive<-which(TestData$Y==1)
trueNegative<-which(TestData$Y==0)

#Count how many true positives and true negatives
ntruePositive<-length(truePositive)
ntrueNegative<-length(trueNegative)

library(class)
##Create the loop
for( i in 1:nK){
  # Step 1: Store K
  storagespace [i,1] <- i

  # Step 2: Run KNN
  PredictingK <- knn(TrainingData[,c(2,3,8:11)], TestData[,c(2,3,8:11)], k =    i,cl = TrainingData$Y )
  # Step 3: Compute Sensitivity
  Sensitivity <- sum(PredictingK[truePositive] == 1) /ntruePositive
  storagespace[i,2] <- Sensitivity

  # Step 4: Compute Specificity
  Specificity <- sum(PredictingK[trueNegative] == 0)/ntrueNegative
  
  
 # Step 5: Compute Geometric Mean
 storagespace[i,2] <- sqrt(Sensitivity*Specificity)



}


ggplot(storagespace, aes(K, GMean)) + geom_line() +
labs(caption = paste("Geometric Mean, Red Dashed Line at K = ",
which.max(storagespace$GMean)), title = "Figure 2.2", y = " ") +
geom_vline(xintercept = which.max(storagespace$GMean), lty = 2,
col = "red")



```


```{r}
# Making confuxion matrix for 20%Test-80%Training Data
knitr::kable(table( PredictingK,TestData$Y), caption= "Table 2.1")

```


```{r,fig.asp = .4}

##Creating 10 folds CV

##How many rows are in our cleaned dengueData
n<-nrow(dengueDataClean)

##Determine how many choices of k
nK <- 50



#Create the folds
poolData<-rep(1:10,ceiling(n/10))

set.seed(123)
foldsHere<-sample(poolData,n)

##Create storage space
storageHere<-data.frame("K"= rep(NA,nK),"GMean"= rep(NA,nK))

#load library class
library(class)

##The loop
# Begin Outer Look
for (k in 1:nK) {
  
  #Step 1: Store K
  storageHere[k,1]<-k
  
  # Step 2: Create a space to store the predictions
  #from 10_fold CV
  storageInner <- data.frame("YHat" = rep(NA,n))
  
  #Begin inner loop
  for (f in 1:10) {
    #Step 2: Find the Data in Fold f
    infold<-which(foldsHere==f)
    
    #Step 3: create training data and test Data
    newTrainingData<-dengueDataClean[-infold,]
    newTestData<-dengueDataClean[infold,]
    
    ##Determine the true positives and true negatives
    NewtruePositive<-which(newTestData$Y==1)
    NewtrueNegative<-which(newTestData$Y==0)

    #Count how many true positives and true negatives
    nNewtruePositive<-length(NewtruePositive)
    nNewtrueNegative<-length(NewtrueNegative)
    
    # Step 4: Make Predictions
   PredictK <- knn(newTrainingData[,c(2,3,8:11)], newTestData[,c(2,3,8:11)], k = k,
    cl = newTrainingData$Y) 
    # Step 5: Store the prediction
    storageInner$YHat[infold] <- as.numeric(as.character(PredictK))
    
  }
   # Step 6: Compute Sensitivity
  sensitivity <- sum(storageInner$YHat[NewtruePositive] == 1) /nNewtruePositive
  

  # Step 7: Compute Specificity
  
  specificity <- sum(storageInner$YHat[NewtrueNegative] == 0)/nNewtrueNegative
  

  # Step 8: Compute Geometric Mean
  storageHere[k,2] <- sqrt(sensitivity*specificity)

 
  
}


ggplot(storageHere, aes(K, GMean)) + geom_line() +
labs(caption = paste("Geometric Mean, Blue Dashed Line at K = ",
which.max(storageHere$GMean)), title = "Figure 2.3", y = " ") +
geom_vline(xintercept = which.max(storageHere$GMean), lty = 2,
col = "blue")


```


```{r}
# Making confuxion matrix for 10-fold CV
knitr::kable(table( PredictK,newTestData$Y), caption= "Table 2.2" )

```


# Section 3: Logistic Regression



```{r}
#Checking class of the variables
sapply(dengueDataClean,class)


#Assigning the correct class to the variables

dengueDataClean$Sex<-as.factor(dengueDataClean$Sex)
dengueDataClean$Vomiting<-as.factor(dengueDataClean$Vomiting)
dengueDataClean$Abdo<-as.factor(dengueDataClean$Abdo)
dengueDataClean$Muco<-as.factor(dengueDataClean$Muco)
dengueDataClean$Skin<-as.factor(dengueDataClean$Skin)
```



```{r, fig.asp = .6}
#Testing for multicolinearity in the cleaned data
#Step 1 - Install necessary packages
suppressMessages(library(caTools))
suppressMessages(library(car))
suppressMessages(library(quantmod))
suppressMessages(library(MASS))
suppressMessages(library(corrplot))
par(mfrow = c(1, 2))


##Step 2 - Create a linear regression model

model_all <- lm(Y ~ ., data=dengueDataClean)  # with all the independent variables in the dataframe

#summary(model_all)

#Step 3 - Use the vif() function

#vif(model_all)

#Step 4 - Visualize VIF Values

vif_values <- vif(model_all)           #create vector of VIF values

barplot(vif_values, main = "Figure 3.1", horiz = TRUE, col = "steelblue") #create horizontal bar chart to display each VIF value

abline(v = 5, lwd = 3, lty = 2)    #add vertical line at 5 as after 5 there is severe correlation

#After plotting the graph, user can does decide which variable to remove i.e not include in model building and check whether the coreesponding R squared value improves.

#Step 5 - Multicollinearity test can be checked by

data_x <- dengueDataClean[,c(2,9,10,11)]                                       # independent variables 

var <- cor(data_x)                                         # independent variables correlation matrix 

var_inv <- ginv(var)                                       # independent variables inverse correlation matrix 

colnames(var_inv) <- colnames(data_x)                      # rename the row names and column names
#rownames(var_inv) <- colnames(data_x)

corrplot(var_inv,method='number',is.corr = F,main = "Figure 3.2")              # visualize the multicollinearity
#{"mode":"full","isActive":false}
```



```{r}
emplogitPlot <- function(x, y, binsize = NULL, ci = FALSE, probit = FALSE,
prob = FALSE, main = NULL, xlab = "", ylab = "", lowess.in = FALSE){
  # x         vector with values of the independent variable
  # y         vector of binary responses
  # binsize   integer value specifying bin size (optional)
  # ci        logical value indicating whether to plot approximate
  #           confidence intervals (not supported as of 02/08/2015)
  # probit    logical value indicating whether to plot probits instead
  #           of logits
  # prob      logical value indicating whether to plot probabilities
  #           without transforming
  #
  # the rest are the familiar plotting options
  
  if(class(y) =="character"){
   y <- as.numeric(as.factor(y))-1
   }
  
  if (length(x) != length(y))
    stop("x and y lengths differ")
  if (any(y < 0 | y > 1))
    stop("y not between 0 and 1")
  if (length(x) < 100 & is.null(binsize))
    stop("Less than 100 observations: specify binsize manually")
  
  if (is.null(binsize)) binsize = min(round(length(x)/10), 50)
  
  if (probit){
    link = qnorm
    if (is.null(main)) main = "Empirical probits"
  } else {
    link = function(x) log(x/(1-x))
    if (is.null(main)) main = "Empirical logits"
  }
  
  sort = order(x)
  x = x[sort]
  y = y[sort]
  a = seq(1, length(x), by=binsize)
  b = c(a[-1] - 1, length(x))
  
  prob = xmean = ns = rep(0, length(a)) # ns is for CIs
  for (i in 1:length(a)){
    range = (a[i]):(b[i])
    prob[i] = mean(y[range])
    xmean[i] = mean(x[range])
    ns[i] = b[i] - a[i] + 1 # for CI 
  }
  
  extreme = (prob == 1 | prob == 0)
  prob[prob == 0] = min(prob[!extreme])
  prob[prob == 1] = max(prob[!extreme])
  
  g = link(prob) # logits (or probits if probit == TRUE)
  
  linear.fit = lm(g[!extreme] ~ xmean[!extreme])
  b0 = linear.fit$coef[1]
  b1 = linear.fit$coef[2]
  
  loess.fit = loess(g[!extreme] ~ xmean[!extreme])
  
  plot(xmean, g, main=main, xlab=xlab, ylab=ylab)
  abline(b0,b1)
  if(lowess.in ==TRUE){
  lines(loess.fit$x, loess.fit$fitted, lwd=2, lty=2)
  }
}
```

```{r, fig.asp = .6}
#log odds for 10-folds CV
par(mfrow = c(2, 2))

#log odds of Y to Age
emplogitPlot(x=newTrainingData$Age, y=newTrainingData$Y, 
             xlab = "Sex of child", 
             ylab = "Log Odds of child having Dengue fever", 
             main = "Figure 3.3")


#Log odds of Y to DayDisease

emplogitPlot(x=newTrainingData$DayDisease, y=newTrainingData$Y, 
             xlab = "Number of days of disease", 
             ylab = "Log Odds of child having Dengue fever", 
             main = "Figure 3.4")


#Log odds of Y to Temp

emplogitPlot(x=newTrainingData$Temp, y=newTrainingData$Y, 
             xlab = "Body temperature of child", 
             ylab = "Log Odds of child having Dengue fever", 
             main = "Figure 3.5")


#Log odds of Y to BMI

emplogitPlot(x=newTrainingData$BMI, y=newTrainingData$Y, 
             xlab = "Body Mass Index of child", 
             ylab = "Log Odds of child having Dengue fever", 
             main = "Figure 3.6")
```



```{r}
#logistic regression with 10folds-CV


model1 <- glm( Y~Sex+Age+DayDisease+Vomiting+Abdo+Muco+Skin+Temp +BMI+Flush+Hepatomegaly+ Rash, data = newTrainingData,
family = "binomial")

knitr::kable( summary(model1)$coefficients, main="Table 3.1")



#Creating thresholds
probabilities1 <- predict(model1, newdata = newTestData, type ="resp")

#Making Prediction
predicted.Y1 <- ifelse(probabilities1 > 0.5, "1", "0")

```



```{r}
# Making confuxion matrix for logistic regression
knitr::kable(table( predicted.Y1,newTestData$Y), caption= "Table 3.2" )
```




